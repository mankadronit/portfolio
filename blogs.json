{"status":"ok","feed":{"url":"https://medium.com/feed/@mankadronit.rm","title":"Stories by Ronit Mankad on Medium","link":"https://medium.com/@mankadronit.rm?source=rss-e453ce09830------2","author":"","description":"Stories by Ronit Mankad on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/2*f-WFv5fwZBljsIsvuUl3Hw.jpeg"},"items":[{"title":"A 5-Step Guide on incorporating Differential Privacy into your Deep Learning models","pubDate":"2019-07-15 17:54:31","link":"https://towardsdatascience.com/a-5-step-guide-on-incorporating-differential-privacy-into-your-deep-learning-models-7861c6c822c4?source=rss-e453ce09830------2","guid":"https://medium.com/p/7861c6c822c4","author":"Ronit Mankad","thumbnail":"","description":"\n<h4>Using PyTorch and Differential Privacy to classify MNIST\u00a0digits</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*NSN1a2xVtV1exzcD8fpzhA.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@euwars?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Farzad Nazifi</a> on\u00a0<a href=\"https://unsplash.com/search/photos/technology?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></figcaption></figure><h3>Introduction</h3>\n<p>By now we all know the benefits of Differential Privacy, how it can protect individual privacy while still providing accurate query results over a large dataset. This post will illustrate how to apply Differential Privacy to the MNIST Digit classification problem and analyze it using a technique called <strong>Private Aggregation of Teacher Ensembles (PATE).</strong></p>\n<h3>Methodology</h3>\n<p>First, we will divide the private data into N number of sets (in this case, 100) and train a classifier on each of the N datasets. These are called <strong>Teacher</strong> classifiers. We will then use the teacher classifiers to predict the labels for our public data. For each image in the public dataset, the most predicted label by the N classifiers will be considered as the true label for that\u00a0image.</p>\n<p>Now, using the predictions of the <strong>Teacher</strong> classifiers as true labels for our public data, we will train a <strong>Student</strong> classifier which can then be used to classify new unseen\u00a0images.</p>\n<figure><img alt=\"Architecture Overview\" src=\"https://cdn-images-1.medium.com/max/700/0*W9Nu6tkIkMAAllg0.png\"><figcaption>Figure 2: Overview of the approach: (1) an ensemble of teachers is trained on disjoint subsets of the sensitive data, (2) a student model is trained on public data labeled using the ensemble.</figcaption></figure><blockquote><em>The MNIST Train data will be considered as private data on which we will train our Teacher models. The Student model obtained by combining the predictions of the teacher models will then be trained on the MNIST Test data (90% of test data will be used to train the model and the remaining 10% will be used to test its accuracy)</em></blockquote>\n<h3>Okay, but where does the privacy part come into\u00a0play?</h3>\n<p>Deep Learning models have a tendency to overfit the training data. Instead of learning general features, neural networks can learn features of an individual which then can be exploited by an adversary to obtain the individual\u2019s private information.</p>\n<p>By not training the Student model directly on the private data, we prevent it from directly learning key individual features of a single person from the dataset. Instead, the generalized features and trends learned by the Teacher models are used to train the\u00a0Student.</p>\n<p>However, there is one small caveat. If the label of an image can be changed by removing the prediction of a single teacher, an adversary can narrow down the search to that\u00a0model.</p>\n<p>To avoid this we add random <a href=\"https://en.wikipedia.org/wiki/Laplace_distribution\"><strong>Laplacian Noise</strong></a><strong> </strong>to the predictions of the teacher models before selecting the most predicted label as the true label for the public data. In this way, we add a bit of randomness and skew the final result so that the true label doesn\u2019t easily change by dropping just one\u00a0teacher.</p>\n<h3>Implementing Differential Privacy using\u00a0PyTorch</h3>\n<h4><strong>Step 1: Loading the\u00a0Data</strong></h4>\n<p>Import the MNIST data from <em>torchvision </em>and<em> </em>define a function to generate the dataloaders.</p>\n<pre><strong>import</strong> <strong>torch</strong><br><br><strong>from</strong> <strong>torchvision</strong> <strong>import</strong> datasets, transforms<br><strong>from</strong> <strong>torch.utils.data</strong> <strong>import</strong> Subset<br><br><em># Transform the image to a tensor and normalize it</em><br>transform = transforms.Compose([transforms.ToTensor(),<br>                                transforms.Normalize((0.5,), (0.5,))])<br><br><em># Load the train and test data by using the transform</em><br>train_data = datasets.MNIST(root='data', train=<strong>True</strong>, download=<strong>True</strong>, transform=transform)<br>test_data = datasets.MNIST(root='data', train=<strong>False</strong>, download=<strong>True</strong>, transform=transform)</pre>\n<pre>num_teachers = 100 <em># Define the num of teachers</em><br>batch_size = 32 <em># Teacher batch size</em><br><br><strong>def</strong> get_data_loaders(train_data, num_teachers):<br><em>\"\"\" Function to create data loaders for the Teacher classifier \"\"\"</em><br>    teacher_loaders = []<br>    data_size = len(train_data) // num_teachers<br><br><strong>for</strong> i <strong>in</strong> range(data_size):<br>        indices = list(range(i*data_size, (i+1)*data_size))<br>        subset_data = Subset(train_data, indices)<br>        loader = torch.utils.data.DataLoader(subset_data, batch_size=batch_size)<br>        teacher_loaders.append(loader)<br><br><strong>return</strong> teacher_loaders<br><br>teacher_loaders = get_data_loaders(train_data, num_teachers)</pre>\n<p>Now, generate the student train and test data by splitting the MNIST test set as discussed above.</p>\n<pre><em># Create the public dataset by using 90% of the Test data as train #data and remaining 10% as test data.</em></pre>\n<pre>student_train_data = Subset(test_data, list(range(9000)))<br>student_test_data = Subset(test_data, list(range(9000, 10000)))<br><br>student_train_loader = torch.utils.data.DataLoader(student_train_data, batch_size=batch_size)<br>student_test_loader = torch.utils.data.DataLoader(student_test_data, batch_size=batch_size)</pre>\n<h4>Step 2: Defining and Training the Teacher\u00a0models</h4>\n<p>Define a simple CNN to classify the MNIST\u00a0digits.</p>\n<pre><strong>import</strong> <strong>torch.nn</strong> <strong>as</strong> <strong>nn</strong><br><strong>import</strong> <strong>torch.nn.functional</strong> <strong>as</strong> <strong>F</strong><br><strong>import</strong> <strong>torch.optim</strong> <strong>as</strong> <strong>optim</strong><br><br><strong>class</strong> <strong>Classifier</strong>(nn.Module):<br><em>\"\"\" A Simple Feed Forward Neural Network. </em><br><em>        A CNN can also be used for this problem </em><br><em>    \"\"\"</em><br><strong>def</strong> __init__(self):<br>        super().__init__()<br><br>        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)<br>        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)<br>        self.conv2_drop = nn.Dropout2d()<br>        self.fc1 = nn.Linear(320, 50)<br>        self.fc2 = nn.Linear(50, 10)<br><br><strong>def</strong> forward(self, x):<br>        x = F.relu(F.max_pool2d(self.conv1(x), 2))<br>        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))<br>        x = x.view(-1, 320)<br>        x = F.relu(self.fc1(x))<br>        x = F.dropout(x, training=self.training)<br>        x = self.fc2(x)<br><strong>return</strong> F.log_softmax(x)</pre>\n<p>Now define the train and predict functions</p>\n<pre><strong>def</strong> train(model, trainloader, criterion, optimizer, epochs=10):<br><em>\"\"\" This function trains a single Classifier model \"\"\"</em><br>    running_loss = 0<br><strong>for</strong> e <strong>in</strong> range(epochs):<br>        model.train()<br><br><strong>for</strong> images, labels <strong>in</strong> trainloader:<br>            optimizer.zero_grad()<br><br>            output = model.forward(images)<br>            loss = criterion(output, labels)<br>            loss.backward()<br>            optimizer.step()<br><br>            running_loss += loss.item()</pre>\n<pre><strong>def</strong> predict(model, dataloader):<br><em>\"\"\" This function predicts labels for a dataset </em><br><em>        given the model and dataloader as inputs. </em><br><em>    \"\"\"</em><br>    outputs = torch.zeros(0, dtype=torch.long)<br>    model.eval()<br><br><strong>for</strong> images, labels <strong>in</strong> dataloader:<br>        output = model.forward(images)<br>        ps = torch.argmax(torch.exp(output), dim=1)<br>        outputs = torch.cat((outputs, ps))<br><br><strong>return</strong> outputs</pre>\n<pre><strong>def</strong> train_models(num_teachers):<br><em>\"\"\" Trains *num_teacher* models (num_teachers being the number of teacher classifiers) \"\"\"</em><br>    models = []<br><strong>for</strong> i <strong>in</strong> range(num_teachers):<br>        model = Classifier()<br>        criterion = nn.NLLLoss()<br>        optimizer = optim.Adam(model.parameters(), lr=0.003)<br>        train(model, teacher_loaders[i], criterion, optimizer)<br>        models.append(model)<br><strong>return</strong> models</pre>\n<pre>models = train_models(num_teachers)</pre>\n<h4>Step 3: Generate the Aggregated Teacher and Student labels by combining the predictions of the Teacher\u00a0models.</h4>\n<p>Now, we need to choose the epsilon value for which we first define the formal definition of Differential Privacy</p>\n<p>This definition does not <em>create</em> differential privacy, instead, it is a measure of how much privacy is afforded by a query M. Specifically, it\u2019s a comparison between running the query M on a database (x) and a parallel database (y). As you remember, parallel databases are defined to be the same as a full database (x) with one entry/person removed.</p>\n<p>Thus, this definition says that FOR ALL parallel databases, the maximum distance between a query on database (x) and the same query on database (y) will be e^epsilon, but that occasionally this constraint won\u2019t hold with probability delta. Thus, this theorem is called \u201cepsilon-delta\u201d differential privacy.</p>\n<h4>How much noise should we\u00a0add?</h4>\n<p>The amount of noise necessary to add to the output of a query is a function of four\u00a0things:</p>\n<ul>\n<li>the type of noise (Gaussian/Laplacian)</li>\n<li>the sensitivity of the query/function</li>\n<li>the desired epsilon\u00a0(\u03b5)</li>\n<li>the desired delta\u00a0(\u03b4)</li>\n</ul>\n<p>Thus, for each type of noise we\u2019re adding, we have a different way of calculating how much to add as a function of sensitivity, epsilon, and delta. We\u2019re going to focus on Laplacian noise.</p>\n<p>Laplacian noise is increased/decreased according to a \u201cscale\u201d parameter b. We choose \u201cb\u201d based on the following formula.</p>\n<p>b = sensitivity(query) /\u00a0epsilon</p>\n<p>In other words, if we set b to be this value, then we know that we will have a privacy leakage of &lt;= epsilon. Furthermore, the nice thing about Laplace is that it guarantees this with delta == 0. There are some tunings where we can have very low epsilon where delta is non-zero, but we\u2019ll ignore them for\u00a0now.</p>\n<pre><strong>import</strong> <strong>numpy</strong> <strong>as</strong> <strong>np</strong><br><br>epsilon = 0.2</pre>\n<pre><strong>def</strong> aggregated_teacher(models, dataloader, epsilon):<br><em>\"\"\" Take predictions from individual teacher model and </em><br><em>        creates the true labels for the student after adding </em><br><em>        laplacian noise to them </em><br><em>    \"\"\"</em><br>    preds = torch.torch.zeros((len(models), 9000), dtype=torch.long)<br><strong>for</strong> i, model <strong>in</strong> enumerate(models):<br>        results = predict(model, dataloader)<br>        preds[i] = results<br><br>    labels = np.array([]).astype(int)<br><strong>for</strong> image_preds <strong>in</strong> np.transpose(preds):<br>        label_counts = np.bincount(image_preds, minlength=10)<br>        beta = 1 / epsilon<br><br><strong>for</strong> i <strong>in</strong> range(len(label_counts)):<br>            label_counts[i] += np.random.laplace(0, beta, 1)<br><br>        new_label = np.argmax(label_counts)<br>        labels = np.append(labels, new_label)<br><br><strong>return</strong> preds.numpy(), labels</pre>\n<pre>teacher_models = models<br>preds, student_labels = aggregated_teacher(teacher_models, student_train_loader, epsilon)</pre>\n<h4>Step 4: Create the Student model and train it using the labels generated in step\u00a03.</h4>\n<pre><strong>def</strong> student_loader(student_train_loader, labels):<br><strong>for</strong> i, (data, _) <strong>in</strong> enumerate(iter(student_train_loader)):<br><strong>yield</strong> data, torch.from_numpy(labels[i*len(data): (i+1)*len(data)])</pre>\n<pre>student_model = Classifier()<br>criterion = nn.NLLLoss()<br>optimizer = optim.Adam(student_model.parameters(), lr=0.003)<br>epochs = 10<br>steps = 0<br>running_loss = 0<br><strong>for</strong> e <strong>in</strong> range(epochs):<br>    student_model.train()<br>    train_loader = student_loader(student_train_loader, student_labels)<br><strong>for</strong> images, labels <strong>in</strong> train_loader:<br>        steps += 1<br><br>        optimizer.zero_grad()<br>        output = student_model.forward(images)<br>        loss = criterion(output, labels)<br>        loss.backward()<br>        optimizer.step()<br><br>        running_loss += loss.item()<br><br><strong>if</strong> steps % 50 == 0:<br>            test_loss = 0<br>            accuracy = 0<br>            student_model.eval()<br><strong>with</strong> torch.no_grad():<br><strong>for</strong> images, labels <strong>in</strong> student_test_loader:<br>                    log_ps = student_model(images)<br>                    test_loss += criterion(log_ps, labels).item()<br><br><em># Accuracy</em><br>                    ps = torch.exp(log_ps)<br>                    top_p, top_class = ps.topk(1, dim=1)<br>                    equals = top_class == labels.view(*top_class.shape)<br>                    accuracy += torch.mean(equals.type(torch.FloatTensor))<br>            student_model.train()<br>            print(\"Epoch: <strong>{}</strong>/<strong>{}</strong>.. \".format(e+1, epochs),<br>                  \"Training Loss: <strong>{:.3f}</strong>.. \".format(running_loss/len(student_train_loader)),<br>                  \"Test Loss: <strong>{:.3f}</strong>.. \".format(test_loss/len(student_test_loader)),<br>                  \"Test Accuracy: <strong>{:.3f}</strong>\".format(accuracy/len(student_test_loader)))<br>            running_loss = 0</pre>\n<p>Here\u2019s a snippet of the <strong>training loss</strong> and <strong>accuracy</strong> achieved.</p>\n<pre>Epoch: 9/10..  Training Loss: 0.035..  Test Loss: 0.206..  Test Accuracy: 0.941<br>Epoch: 9/10..  Training Loss: 0.034..  Test Loss: 0.196..  Test Accuracy: 0.949<br>Epoch: 10/10..  Training Loss: 0.048..  Test Loss: 0.204..  Test Accuracy: 0.943<br>Epoch: 10/10..  Training Loss: 0.046..  Test Loss: 0.203..  Test Accuracy: 0.943<br>Epoch: 10/10..  Training Loss: 0.045..  Test Loss: 0.203..  Test Accuracy: 0.945<br>Epoch: 10/10..  Training Loss: 0.049..  Test Loss: 0.207..  Test Accuracy: 0.946<br>Epoch: 10/10..  Training Loss: 0.032..  Test Loss: 0.228..  Test Accuracy: 0.941<br>Epoch: 10/10..  Training Loss: 0.030..  Test Loss: 0.252..  Test Accuracy: 0.939</pre>\n<h4>Step 5: Let\u2019s Perform PATE Analysis on the student labels generated by the Aggregated Teacher</h4>\n<pre><strong>from</strong> <strong>syft.frameworks.torch.differential_privacy</strong> <strong>import</strong> pate<br><br>data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=student_labels, noise_eps=epsilon, delta=1e-5)<br>print(\"Data Independent Epsilon:\", data_ind_eps)<br>print(\"Data Dependent Epsilon:\", data_dep_eps)</pre>\n<p>Output:</p>\n<pre>Data Independent Epsilon: 1451.5129254649705<br>Data Dependent Epsilon: 4.34002697554237</pre>\n<p>The pate.perform_analysis method returns two values - a data-independent epsilon and a data-dependent epsilon. The data-dependent epsilon is the epsilon value obtained by looking at how much the teachers agree with each other. In a way, the PATE analysis rewards the user for building teacher models which agree with each other because it becomes harder to leak information and track individual information.</p>\n<h3>Conclusion</h3>\n<p>Using the Student-Teacher architecture guided by the PATE analysis method is a great way to introduce Differential Privacy to your deep learning models. However, Differential Privacy is still in its early stage and as more research in the space occurs more sophisticated methods will be developed to reduce the privacy-accuracy tradeoff and the downside that differential privacy only really performs well on large datasets.</p>\n<h3>References</h3>\n<p>[1] Dwork, C. and Roth, A. <a href=\"http://www.nowpublishers.com/article/Details/TCS-042\">The algorithmic foundations of differential privacy </a>(2014), <em>Foundations and Trends\u00ae in Theoretical Computer Science</em>, <em>9</em>(3\u20134), pp.211\u2013407.</p>\n<p>[2] Abadi, Martin, et al, <a href=\"https://dl.acm.org/citation.cfm?id=2978318\">Deep learning with differential privacy</a> (2016), <em>Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</em>. ACM,\u00a02016.</p>\n<p>[3] Figure 1, Photo by <a href=\"https://unsplash.com/@euwars?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Farzad Nazifi</a> on\u00a0<a href=\"https://unsplash.com/search/photos/technology?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></p>\n<p>[4] Figure 2, Nicolas Papernot, et al, <a href=\"https://arxiv.org/abs/1802.08908\">Scalable Private Learning with PATE</a>(2018), Published as a conference paper at ICLR\u00a02018</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7861c6c822c4\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://towardsdatascience.com/a-5-step-guide-on-incorporating-differential-privacy-into-your-deep-learning-models-7861c6c822c4\">A 5-Step Guide on incorporating Differential Privacy into your Deep Learning models</a> was originally published in <a href=\"https://towardsdatascience.com/\">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h4>Using PyTorch and Differential Privacy to classify MNIST\u00a0digits</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*NSN1a2xVtV1exzcD8fpzhA.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@euwars?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Farzad Nazifi</a> on\u00a0<a href=\"https://unsplash.com/search/photos/technology?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></figcaption></figure><h3>Introduction</h3>\n<p>By now we all know the benefits of Differential Privacy, how it can protect individual privacy while still providing accurate query results over a large dataset. This post will illustrate how to apply Differential Privacy to the MNIST Digit classification problem and analyze it using a technique called <strong>Private Aggregation of Teacher Ensembles (PATE).</strong></p>\n<h3>Methodology</h3>\n<p>First, we will divide the private data into N number of sets (in this case, 100) and train a classifier on each of the N datasets. These are called <strong>Teacher</strong> classifiers. We will then use the teacher classifiers to predict the labels for our public data. For each image in the public dataset, the most predicted label by the N classifiers will be considered as the true label for that\u00a0image.</p>\n<p>Now, using the predictions of the <strong>Teacher</strong> classifiers as true labels for our public data, we will train a <strong>Student</strong> classifier which can then be used to classify new unseen\u00a0images.</p>\n<figure><img alt=\"Architecture Overview\" src=\"https://cdn-images-1.medium.com/max/700/0*W9Nu6tkIkMAAllg0.png\"><figcaption>Figure 2: Overview of the approach: (1) an ensemble of teachers is trained on disjoint subsets of the sensitive data, (2) a student model is trained on public data labeled using the ensemble.</figcaption></figure><blockquote><em>The MNIST Train data will be considered as private data on which we will train our Teacher models. The Student model obtained by combining the predictions of the teacher models will then be trained on the MNIST Test data (90% of test data will be used to train the model and the remaining 10% will be used to test its accuracy)</em></blockquote>\n<h3>Okay, but where does the privacy part come into\u00a0play?</h3>\n<p>Deep Learning models have a tendency to overfit the training data. Instead of learning general features, neural networks can learn features of an individual which then can be exploited by an adversary to obtain the individual\u2019s private information.</p>\n<p>By not training the Student model directly on the private data, we prevent it from directly learning key individual features of a single person from the dataset. Instead, the generalized features and trends learned by the Teacher models are used to train the\u00a0Student.</p>\n<p>However, there is one small caveat. If the label of an image can be changed by removing the prediction of a single teacher, an adversary can narrow down the search to that\u00a0model.</p>\n<p>To avoid this we add random <a href=\"https://en.wikipedia.org/wiki/Laplace_distribution\"><strong>Laplacian Noise</strong></a><strong> </strong>to the predictions of the teacher models before selecting the most predicted label as the true label for the public data. In this way, we add a bit of randomness and skew the final result so that the true label doesn\u2019t easily change by dropping just one\u00a0teacher.</p>\n<h3>Implementing Differential Privacy using\u00a0PyTorch</h3>\n<h4><strong>Step 1: Loading the\u00a0Data</strong></h4>\n<p>Import the MNIST data from <em>torchvision </em>and<em> </em>define a function to generate the dataloaders.</p>\n<pre><strong>import</strong> <strong>torch</strong><br><br><strong>from</strong> <strong>torchvision</strong> <strong>import</strong> datasets, transforms<br><strong>from</strong> <strong>torch.utils.data</strong> <strong>import</strong> Subset<br><br><em># Transform the image to a tensor and normalize it</em><br>transform = transforms.Compose([transforms.ToTensor(),<br>                                transforms.Normalize((0.5,), (0.5,))])<br><br><em># Load the train and test data by using the transform</em><br>train_data = datasets.MNIST(root='data', train=<strong>True</strong>, download=<strong>True</strong>, transform=transform)<br>test_data = datasets.MNIST(root='data', train=<strong>False</strong>, download=<strong>True</strong>, transform=transform)</pre>\n<pre>num_teachers = 100 <em># Define the num of teachers</em><br>batch_size = 32 <em># Teacher batch size</em><br><br><strong>def</strong> get_data_loaders(train_data, num_teachers):<br><em>\"\"\" Function to create data loaders for the Teacher classifier \"\"\"</em><br>    teacher_loaders = []<br>    data_size = len(train_data) // num_teachers<br><br><strong>for</strong> i <strong>in</strong> range(data_size):<br>        indices = list(range(i*data_size, (i+1)*data_size))<br>        subset_data = Subset(train_data, indices)<br>        loader = torch.utils.data.DataLoader(subset_data, batch_size=batch_size)<br>        teacher_loaders.append(loader)<br><br><strong>return</strong> teacher_loaders<br><br>teacher_loaders = get_data_loaders(train_data, num_teachers)</pre>\n<p>Now, generate the student train and test data by splitting the MNIST test set as discussed above.</p>\n<pre><em># Create the public dataset by using 90% of the Test data as train #data and remaining 10% as test data.</em></pre>\n<pre>student_train_data = Subset(test_data, list(range(9000)))<br>student_test_data = Subset(test_data, list(range(9000, 10000)))<br><br>student_train_loader = torch.utils.data.DataLoader(student_train_data, batch_size=batch_size)<br>student_test_loader = torch.utils.data.DataLoader(student_test_data, batch_size=batch_size)</pre>\n<h4>Step 2: Defining and Training the Teacher\u00a0models</h4>\n<p>Define a simple CNN to classify the MNIST\u00a0digits.</p>\n<pre><strong>import</strong> <strong>torch.nn</strong> <strong>as</strong> <strong>nn</strong><br><strong>import</strong> <strong>torch.nn.functional</strong> <strong>as</strong> <strong>F</strong><br><strong>import</strong> <strong>torch.optim</strong> <strong>as</strong> <strong>optim</strong><br><br><strong>class</strong> <strong>Classifier</strong>(nn.Module):<br><em>\"\"\" A Simple Feed Forward Neural Network. </em><br><em>        A CNN can also be used for this problem </em><br><em>    \"\"\"</em><br><strong>def</strong> __init__(self):<br>        super().__init__()<br><br>        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)<br>        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)<br>        self.conv2_drop = nn.Dropout2d()<br>        self.fc1 = nn.Linear(320, 50)<br>        self.fc2 = nn.Linear(50, 10)<br><br><strong>def</strong> forward(self, x):<br>        x = F.relu(F.max_pool2d(self.conv1(x), 2))<br>        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))<br>        x = x.view(-1, 320)<br>        x = F.relu(self.fc1(x))<br>        x = F.dropout(x, training=self.training)<br>        x = self.fc2(x)<br><strong>return</strong> F.log_softmax(x)</pre>\n<p>Now define the train and predict functions</p>\n<pre><strong>def</strong> train(model, trainloader, criterion, optimizer, epochs=10):<br><em>\"\"\" This function trains a single Classifier model \"\"\"</em><br>    running_loss = 0<br><strong>for</strong> e <strong>in</strong> range(epochs):<br>        model.train()<br><br><strong>for</strong> images, labels <strong>in</strong> trainloader:<br>            optimizer.zero_grad()<br><br>            output = model.forward(images)<br>            loss = criterion(output, labels)<br>            loss.backward()<br>            optimizer.step()<br><br>            running_loss += loss.item()</pre>\n<pre><strong>def</strong> predict(model, dataloader):<br><em>\"\"\" This function predicts labels for a dataset </em><br><em>        given the model and dataloader as inputs. </em><br><em>    \"\"\"</em><br>    outputs = torch.zeros(0, dtype=torch.long)<br>    model.eval()<br><br><strong>for</strong> images, labels <strong>in</strong> dataloader:<br>        output = model.forward(images)<br>        ps = torch.argmax(torch.exp(output), dim=1)<br>        outputs = torch.cat((outputs, ps))<br><br><strong>return</strong> outputs</pre>\n<pre><strong>def</strong> train_models(num_teachers):<br><em>\"\"\" Trains *num_teacher* models (num_teachers being the number of teacher classifiers) \"\"\"</em><br>    models = []<br><strong>for</strong> i <strong>in</strong> range(num_teachers):<br>        model = Classifier()<br>        criterion = nn.NLLLoss()<br>        optimizer = optim.Adam(model.parameters(), lr=0.003)<br>        train(model, teacher_loaders[i], criterion, optimizer)<br>        models.append(model)<br><strong>return</strong> models</pre>\n<pre>models = train_models(num_teachers)</pre>\n<h4>Step 3: Generate the Aggregated Teacher and Student labels by combining the predictions of the Teacher\u00a0models.</h4>\n<p>Now, we need to choose the epsilon value for which we first define the formal definition of Differential Privacy</p>\n<p>This definition does not <em>create</em> differential privacy, instead, it is a measure of how much privacy is afforded by a query M. Specifically, it\u2019s a comparison between running the query M on a database (x) and a parallel database (y). As you remember, parallel databases are defined to be the same as a full database (x) with one entry/person removed.</p>\n<p>Thus, this definition says that FOR ALL parallel databases, the maximum distance between a query on database (x) and the same query on database (y) will be e^epsilon, but that occasionally this constraint won\u2019t hold with probability delta. Thus, this theorem is called \u201cepsilon-delta\u201d differential privacy.</p>\n<h4>How much noise should we\u00a0add?</h4>\n<p>The amount of noise necessary to add to the output of a query is a function of four\u00a0things:</p>\n<ul>\n<li>the type of noise (Gaussian/Laplacian)</li>\n<li>the sensitivity of the query/function</li>\n<li>the desired epsilon\u00a0(\u03b5)</li>\n<li>the desired delta\u00a0(\u03b4)</li>\n</ul>\n<p>Thus, for each type of noise we\u2019re adding, we have a different way of calculating how much to add as a function of sensitivity, epsilon, and delta. We\u2019re going to focus on Laplacian noise.</p>\n<p>Laplacian noise is increased/decreased according to a \u201cscale\u201d parameter b. We choose \u201cb\u201d based on the following formula.</p>\n<p>b = sensitivity(query) /\u00a0epsilon</p>\n<p>In other words, if we set b to be this value, then we know that we will have a privacy leakage of &lt;= epsilon. Furthermore, the nice thing about Laplace is that it guarantees this with delta == 0. There are some tunings where we can have very low epsilon where delta is non-zero, but we\u2019ll ignore them for\u00a0now.</p>\n<pre><strong>import</strong> <strong>numpy</strong> <strong>as</strong> <strong>np</strong><br><br>epsilon = 0.2</pre>\n<pre><strong>def</strong> aggregated_teacher(models, dataloader, epsilon):<br><em>\"\"\" Take predictions from individual teacher model and </em><br><em>        creates the true labels for the student after adding </em><br><em>        laplacian noise to them </em><br><em>    \"\"\"</em><br>    preds = torch.torch.zeros((len(models), 9000), dtype=torch.long)<br><strong>for</strong> i, model <strong>in</strong> enumerate(models):<br>        results = predict(model, dataloader)<br>        preds[i] = results<br><br>    labels = np.array([]).astype(int)<br><strong>for</strong> image_preds <strong>in</strong> np.transpose(preds):<br>        label_counts = np.bincount(image_preds, minlength=10)<br>        beta = 1 / epsilon<br><br><strong>for</strong> i <strong>in</strong> range(len(label_counts)):<br>            label_counts[i] += np.random.laplace(0, beta, 1)<br><br>        new_label = np.argmax(label_counts)<br>        labels = np.append(labels, new_label)<br><br><strong>return</strong> preds.numpy(), labels</pre>\n<pre>teacher_models = models<br>preds, student_labels = aggregated_teacher(teacher_models, student_train_loader, epsilon)</pre>\n<h4>Step 4: Create the Student model and train it using the labels generated in step\u00a03.</h4>\n<pre><strong>def</strong> student_loader(student_train_loader, labels):<br><strong>for</strong> i, (data, _) <strong>in</strong> enumerate(iter(student_train_loader)):<br><strong>yield</strong> data, torch.from_numpy(labels[i*len(data): (i+1)*len(data)])</pre>\n<pre>student_model = Classifier()<br>criterion = nn.NLLLoss()<br>optimizer = optim.Adam(student_model.parameters(), lr=0.003)<br>epochs = 10<br>steps = 0<br>running_loss = 0<br><strong>for</strong> e <strong>in</strong> range(epochs):<br>    student_model.train()<br>    train_loader = student_loader(student_train_loader, student_labels)<br><strong>for</strong> images, labels <strong>in</strong> train_loader:<br>        steps += 1<br><br>        optimizer.zero_grad()<br>        output = student_model.forward(images)<br>        loss = criterion(output, labels)<br>        loss.backward()<br>        optimizer.step()<br><br>        running_loss += loss.item()<br><br><strong>if</strong> steps % 50 == 0:<br>            test_loss = 0<br>            accuracy = 0<br>            student_model.eval()<br><strong>with</strong> torch.no_grad():<br><strong>for</strong> images, labels <strong>in</strong> student_test_loader:<br>                    log_ps = student_model(images)<br>                    test_loss += criterion(log_ps, labels).item()<br><br><em># Accuracy</em><br>                    ps = torch.exp(log_ps)<br>                    top_p, top_class = ps.topk(1, dim=1)<br>                    equals = top_class == labels.view(*top_class.shape)<br>                    accuracy += torch.mean(equals.type(torch.FloatTensor))<br>            student_model.train()<br>            print(\"Epoch: <strong>{}</strong>/<strong>{}</strong>.. \".format(e+1, epochs),<br>                  \"Training Loss: <strong>{:.3f}</strong>.. \".format(running_loss/len(student_train_loader)),<br>                  \"Test Loss: <strong>{:.3f}</strong>.. \".format(test_loss/len(student_test_loader)),<br>                  \"Test Accuracy: <strong>{:.3f}</strong>\".format(accuracy/len(student_test_loader)))<br>            running_loss = 0</pre>\n<p>Here\u2019s a snippet of the <strong>training loss</strong> and <strong>accuracy</strong> achieved.</p>\n<pre>Epoch: 9/10..  Training Loss: 0.035..  Test Loss: 0.206..  Test Accuracy: 0.941<br>Epoch: 9/10..  Training Loss: 0.034..  Test Loss: 0.196..  Test Accuracy: 0.949<br>Epoch: 10/10..  Training Loss: 0.048..  Test Loss: 0.204..  Test Accuracy: 0.943<br>Epoch: 10/10..  Training Loss: 0.046..  Test Loss: 0.203..  Test Accuracy: 0.943<br>Epoch: 10/10..  Training Loss: 0.045..  Test Loss: 0.203..  Test Accuracy: 0.945<br>Epoch: 10/10..  Training Loss: 0.049..  Test Loss: 0.207..  Test Accuracy: 0.946<br>Epoch: 10/10..  Training Loss: 0.032..  Test Loss: 0.228..  Test Accuracy: 0.941<br>Epoch: 10/10..  Training Loss: 0.030..  Test Loss: 0.252..  Test Accuracy: 0.939</pre>\n<h4>Step 5: Let\u2019s Perform PATE Analysis on the student labels generated by the Aggregated Teacher</h4>\n<pre><strong>from</strong> <strong>syft.frameworks.torch.differential_privacy</strong> <strong>import</strong> pate<br><br>data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=student_labels, noise_eps=epsilon, delta=1e-5)<br>print(\"Data Independent Epsilon:\", data_ind_eps)<br>print(\"Data Dependent Epsilon:\", data_dep_eps)</pre>\n<p>Output:</p>\n<pre>Data Independent Epsilon: 1451.5129254649705<br>Data Dependent Epsilon: 4.34002697554237</pre>\n<p>The pate.perform_analysis method returns two values - a data-independent epsilon and a data-dependent epsilon. The data-dependent epsilon is the epsilon value obtained by looking at how much the teachers agree with each other. In a way, the PATE analysis rewards the user for building teacher models which agree with each other because it becomes harder to leak information and track individual information.</p>\n<h3>Conclusion</h3>\n<p>Using the Student-Teacher architecture guided by the PATE analysis method is a great way to introduce Differential Privacy to your deep learning models. However, Differential Privacy is still in its early stage and as more research in the space occurs more sophisticated methods will be developed to reduce the privacy-accuracy tradeoff and the downside that differential privacy only really performs well on large datasets.</p>\n<h3>References</h3>\n<p>[1] Dwork, C. and Roth, A. <a href=\"http://www.nowpublishers.com/article/Details/TCS-042\">The algorithmic foundations of differential privacy </a>(2014), <em>Foundations and Trends\u00ae in Theoretical Computer Science</em>, <em>9</em>(3\u20134), pp.211\u2013407.</p>\n<p>[2] Abadi, Martin, et al, <a href=\"https://dl.acm.org/citation.cfm?id=2978318\">Deep learning with differential privacy</a> (2016), <em>Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</em>. ACM,\u00a02016.</p>\n<p>[3] Figure 1, Photo by <a href=\"https://unsplash.com/@euwars?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Farzad Nazifi</a> on\u00a0<a href=\"https://unsplash.com/search/photos/technology?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></p>\n<p>[4] Figure 2, Nicolas Papernot, et al, <a href=\"https://arxiv.org/abs/1802.08908\">Scalable Private Learning with PATE</a>(2018), Published as a conference paper at ICLR\u00a02018</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7861c6c822c4\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://towardsdatascience.com/a-5-step-guide-on-incorporating-differential-privacy-into-your-deep-learning-models-7861c6c822c4\">A 5-Step Guide on incorporating Differential Privacy into your Deep Learning models</a> was originally published in <a href=\"https://towardsdatascience.com/\">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["pytorch","data-science","machine-learning","privacy","deep-learning"]},{"title":"Differential Privacy for Deep Learning","pubDate":"2019-06-20 12:08:49","link":"https://medium.com/secure-and-private-ai-writing-challenge/differential-privacy-for-deep-learning-fbf9a700643c?source=rss-e453ce09830------2","guid":"https://medium.com/p/fbf9a700643c","author":"Ronit Mankad","thumbnail":"","description":"\n<h4>Using Differential Privacy to classify MNIST Digits and perform PATE Analysis on the\u00a0model</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*uWSxNeOVma_Iqd6jns6t-A.jpeg\"><figcaption>Figure 1</figcaption></figure><h3>Introduction</h3>\n<p>By now we all know the benefits of Differential Privacy, how it can protect individual privacy while still providing accurate query results over a large dataset. This post will illustrate how to apply Differential Privacy to the MNIST Digit classification problem and analyze it using a technique called <strong>Private Aggregation of Teacher Ensembles (PATE).</strong></p>\n<h3><strong>Methodology</strong></h3>\n<p>First, we will divide the private data into N number of sets (in this case, 100) and train a classifier on each of the N datasets. These are called <strong>Teacher</strong> classifiers. We will then use the teacher classifiers to predict the labels for our public data. For each image in the public dataset, the most predicted label by the N classifiers will be considered as the true label for that\u00a0image.</p>\n<p>Now, using the predictions of the <strong>Teacher</strong> classifiers as true labels for our public data, we will train a <strong>Student</strong> classifier which can then be used to classify new unseen\u00a0images.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*f_TRmFD2ScafQk01iROSBw.png\"><figcaption>Figure 2: Overview of the approach: (1) an ensemble of teachers is trained on disjoint subsets of the sensitive data, (2) a student model is trained on public data labeled using the ensemble.</figcaption></figure><blockquote>The MNIST Train data will be considered as private data on which we will train our Teacher models. The Student model obtained by combining the predictions of the teacher models will then be trained on the MNIST Test data (90% of test data will be used to train the model and the remaining 10% will be used to test its accuracy)</blockquote>\n<h3>Okay, but where does the privacy part come into\u00a0play?</h3>\n<p>Deep Learning models have a tendency to overfit the training data. Instead of learning general features, neural networks can learn features of an individual which then can be exploited by an adversary to obtain the individual\u2019s private information.</p>\n<p>By not training the Student model directly on the private data, we prevent it from directly learning key individual features of a single person from the dataset. Instead, the generalized features and trends learned by the Teacher models are used to train the\u00a0Student.</p>\n<p>However, there is one small caveat. If the label of an image can be changed by removing the prediction of a single teacher, an adversary can narrow down the search to that\u00a0model.</p>\n<p>To avoid this we add random <a href=\"https://en.wikipedia.org/wiki/Laplace_distribution\"><strong>Laplacian Noise</strong></a><strong> </strong>to the predictions of the teacher models before selecting the most predicted label as the true label for the public data. In this way, we add a bit of randomness and skew the final result so that the true label doesn\u2019t easily change by dropping just one\u00a0teacher.</p>\n<h3>Implementing Differential Privacy using\u00a0PyTorch</h3>\n<a href=\"https://medium.com/media/72d8e2348a28cc3efd58b96cc5d43365/href\">https://medium.com/media/72d8e2348a28cc3efd58b96cc5d43365/href</a><h3>Conclusion</h3>\n<p>Using the Student-Teacher architecture guided by the PATE analysis method is a great way to introduce Differential Privacy to your deep learning models. However, Differential Privacy is still in its early stage and as more research in the space occurs more sophisticated methods will be developed to reduce the privacy-accuracy tradeoff and the downside that differential privacy only really performs well on large datasets.</p>\n<h3>References</h3>\n<p>[1] Dwork, C. and Roth, A. <a href=\"http://www.nowpublishers.com/article/Details/TCS-042\">The algorithmic foundations of differential privacy </a>(2014), <em>Foundations and Trends\u00ae in Theoretical Computer Science</em>, <em>9</em>(3\u20134), pp.211\u2013407.</p>\n<p>[2] Abadi, Martin, et al, <a href=\"https://dl.acm.org/citation.cfm?id=2978318\">Deep learning with differential privacy</a> (2016), <em>Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</em>. ACM,\u00a02016.</p>\n<p>[3] Figure 1, <a href=\"https://unsplash.com/photos/uPXs5Vx5bIg\">Unsplash</a>\u00a0(2016)</p>\n<p>[4] Figure 2, Nicolas Papernot, et al, <a href=\"https://arxiv.org/abs/1802.08908\">Scalable Private Learning with PATE </a>(2018), Published as a conference paper at ICLR\u00a02018</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fbf9a700643c\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/secure-and-private-ai-writing-challenge/differential-privacy-for-deep-learning-fbf9a700643c\">Differential Privacy for Deep Learning</a> was originally published in <a href=\"https://medium.com/secure-and-private-ai-writing-challenge\">Secure and Private AI Writing Challenge</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h4>Using Differential Privacy to classify MNIST Digits and perform PATE Analysis on the\u00a0model</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*uWSxNeOVma_Iqd6jns6t-A.jpeg\"><figcaption>Figure 1</figcaption></figure><h3>Introduction</h3>\n<p>By now we all know the benefits of Differential Privacy, how it can protect individual privacy while still providing accurate query results over a large dataset. This post will illustrate how to apply Differential Privacy to the MNIST Digit classification problem and analyze it using a technique called <strong>Private Aggregation of Teacher Ensembles (PATE).</strong></p>\n<h3><strong>Methodology</strong></h3>\n<p>First, we will divide the private data into N number of sets (in this case, 100) and train a classifier on each of the N datasets. These are called <strong>Teacher</strong> classifiers. We will then use the teacher classifiers to predict the labels for our public data. For each image in the public dataset, the most predicted label by the N classifiers will be considered as the true label for that\u00a0image.</p>\n<p>Now, using the predictions of the <strong>Teacher</strong> classifiers as true labels for our public data, we will train a <strong>Student</strong> classifier which can then be used to classify new unseen\u00a0images.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*f_TRmFD2ScafQk01iROSBw.png\"><figcaption>Figure 2: Overview of the approach: (1) an ensemble of teachers is trained on disjoint subsets of the sensitive data, (2) a student model is trained on public data labeled using the ensemble.</figcaption></figure><blockquote>The MNIST Train data will be considered as private data on which we will train our Teacher models. The Student model obtained by combining the predictions of the teacher models will then be trained on the MNIST Test data (90% of test data will be used to train the model and the remaining 10% will be used to test its accuracy)</blockquote>\n<h3>Okay, but where does the privacy part come into\u00a0play?</h3>\n<p>Deep Learning models have a tendency to overfit the training data. Instead of learning general features, neural networks can learn features of an individual which then can be exploited by an adversary to obtain the individual\u2019s private information.</p>\n<p>By not training the Student model directly on the private data, we prevent it from directly learning key individual features of a single person from the dataset. Instead, the generalized features and trends learned by the Teacher models are used to train the\u00a0Student.</p>\n<p>However, there is one small caveat. If the label of an image can be changed by removing the prediction of a single teacher, an adversary can narrow down the search to that\u00a0model.</p>\n<p>To avoid this we add random <a href=\"https://en.wikipedia.org/wiki/Laplace_distribution\"><strong>Laplacian Noise</strong></a><strong> </strong>to the predictions of the teacher models before selecting the most predicted label as the true label for the public data. In this way, we add a bit of randomness and skew the final result so that the true label doesn\u2019t easily change by dropping just one\u00a0teacher.</p>\n<h3>Implementing Differential Privacy using\u00a0PyTorch</h3>\n<a href=\"https://medium.com/media/72d8e2348a28cc3efd58b96cc5d43365/href\">https://medium.com/media/72d8e2348a28cc3efd58b96cc5d43365/href</a><h3>Conclusion</h3>\n<p>Using the Student-Teacher architecture guided by the PATE analysis method is a great way to introduce Differential Privacy to your deep learning models. However, Differential Privacy is still in its early stage and as more research in the space occurs more sophisticated methods will be developed to reduce the privacy-accuracy tradeoff and the downside that differential privacy only really performs well on large datasets.</p>\n<h3>References</h3>\n<p>[1] Dwork, C. and Roth, A. <a href=\"http://www.nowpublishers.com/article/Details/TCS-042\">The algorithmic foundations of differential privacy </a>(2014), <em>Foundations and Trends\u00ae in Theoretical Computer Science</em>, <em>9</em>(3\u20134), pp.211\u2013407.</p>\n<p>[2] Abadi, Martin, et al, <a href=\"https://dl.acm.org/citation.cfm?id=2978318\">Deep learning with differential privacy</a> (2016), <em>Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</em>. ACM,\u00a02016.</p>\n<p>[3] Figure 1, <a href=\"https://unsplash.com/photos/uPXs5Vx5bIg\">Unsplash</a>\u00a0(2016)</p>\n<p>[4] Figure 2, Nicolas Papernot, et al, <a href=\"https://arxiv.org/abs/1802.08908\">Scalable Private Learning with PATE </a>(2018), Published as a conference paper at ICLR\u00a02018</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fbf9a700643c\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/secure-and-private-ai-writing-challenge/differential-privacy-for-deep-learning-fbf9a700643c\">Differential Privacy for Deep Learning</a> was originally published in <a href=\"https://medium.com/secure-and-private-ai-writing-challenge\">Secure and Private AI Writing Challenge</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["technology","data-science","artificial-intelligence","machine-learning","deep-learning"]}]}